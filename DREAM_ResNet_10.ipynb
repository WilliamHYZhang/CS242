{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APnvXag_EsLW",
        "outputId": "d77308ea-fe88-4490-f20d-21234b8e4581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Tiny ImageNet dataset already exists.\n",
            "Batch size: 1024\n",
            "Using device: cuda\n",
            "Loading Teacher Model (ResNet-101)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-18e4617785df>:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher_model.load_state_dict(torch.load('/content/best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "             ReLU-22           [-1, 64, 56, 56]               0\n",
            "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
            "             ReLU-25          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
            "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
            "             ReLU-29           [-1, 64, 56, 56]               0\n",
            "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
            "             ReLU-32           [-1, 64, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-47          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-57          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "             ReLU-61          [-1, 128, 28, 28]               0\n",
            "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "             ReLU-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-67          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
            "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
            "             ReLU-71          [-1, 128, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-77          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
            "             ReLU-81          [-1, 256, 28, 28]               0\n",
            "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
            "             ReLU-84          [-1, 256, 14, 14]               0\n",
            "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
            "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-89         [-1, 1024, 14, 14]               0\n",
            "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
            "             ReLU-93          [-1, 256, 14, 14]               0\n",
            "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
            "             ReLU-96          [-1, 256, 14, 14]               0\n",
            "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-99         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
            "            ReLU-103          [-1, 256, 14, 14]               0\n",
            "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
            "            ReLU-106          [-1, 256, 14, 14]               0\n",
            "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-109         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
            "            ReLU-113          [-1, 256, 14, 14]               0\n",
            "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
            "            ReLU-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-119         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
            "            ReLU-123          [-1, 256, 14, 14]               0\n",
            "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
            "            ReLU-126          [-1, 256, 14, 14]               0\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
            "            ReLU-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "            ReLU-136          [-1, 256, 14, 14]               0\n",
            "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-139         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
            "            ReLU-143          [-1, 256, 14, 14]               0\n",
            "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
            "            ReLU-146          [-1, 256, 14, 14]               0\n",
            "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-149         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
            "            ReLU-153          [-1, 256, 14, 14]               0\n",
            "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
            "            ReLU-156          [-1, 256, 14, 14]               0\n",
            "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-159         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
            "            ReLU-163          [-1, 256, 14, 14]               0\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "            ReLU-166          [-1, 256, 14, 14]               0\n",
            "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-169         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
            "            ReLU-173          [-1, 256, 14, 14]               0\n",
            "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
            "            ReLU-176          [-1, 256, 14, 14]               0\n",
            "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-179         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "            ReLU-183          [-1, 256, 14, 14]               0\n",
            "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
            "            ReLU-186          [-1, 256, 14, 14]               0\n",
            "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-189         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
            "            ReLU-193          [-1, 256, 14, 14]               0\n",
            "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
            "            ReLU-196          [-1, 256, 14, 14]               0\n",
            "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-199         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
            "            ReLU-203          [-1, 256, 14, 14]               0\n",
            "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
            "            ReLU-206          [-1, 256, 14, 14]               0\n",
            "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-209         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
            "            ReLU-213          [-1, 256, 14, 14]               0\n",
            "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
            "            ReLU-216          [-1, 256, 14, 14]               0\n",
            "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-219         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
            "            ReLU-223          [-1, 256, 14, 14]               0\n",
            "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
            "            ReLU-226          [-1, 256, 14, 14]               0\n",
            "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-229         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
            "            ReLU-233          [-1, 256, 14, 14]               0\n",
            "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
            "            ReLU-236          [-1, 256, 14, 14]               0\n",
            "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-239         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
            "            ReLU-243          [-1, 256, 14, 14]               0\n",
            "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
            "            ReLU-246          [-1, 256, 14, 14]               0\n",
            "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-249         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
            "            ReLU-253          [-1, 256, 14, 14]               0\n",
            "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
            "            ReLU-256          [-1, 256, 14, 14]               0\n",
            "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-259         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
            "            ReLU-263          [-1, 256, 14, 14]               0\n",
            "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
            "            ReLU-266          [-1, 256, 14, 14]               0\n",
            "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-269         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
            "            ReLU-273          [-1, 256, 14, 14]               0\n",
            "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
            "            ReLU-276          [-1, 256, 14, 14]               0\n",
            "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-279         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
            "            ReLU-283          [-1, 256, 14, 14]               0\n",
            "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
            "            ReLU-286          [-1, 256, 14, 14]               0\n",
            "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-289         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
            "            ReLU-293          [-1, 256, 14, 14]               0\n",
            "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
            "            ReLU-296          [-1, 256, 14, 14]               0\n",
            "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-299         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
            "            ReLU-303          [-1, 256, 14, 14]               0\n",
            "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
            "            ReLU-306          [-1, 256, 14, 14]               0\n",
            "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-309         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-313          [-1, 512, 14, 14]               0\n",
            "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-316            [-1, 512, 7, 7]               0\n",
            "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-321           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-322           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-325            [-1, 512, 7, 7]               0\n",
            "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-328            [-1, 512, 7, 7]               0\n",
            "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-331           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-332           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-335            [-1, 512, 7, 7]               0\n",
            "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-338            [-1, 512, 7, 7]               0\n",
            "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-341           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-342           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
            "          Linear-344                  [-1, 200]         409,800\n",
            "================================================================\n",
            "Total params: 42,909,960\n",
            "Trainable params: 42,909,960\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 429.72\n",
            "Params size (MB): 163.69\n",
            "Estimated Total Size (MB): 593.99\n",
            "----------------------------------------------------------------\n",
            "Loading TA Model (ResNet-34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-18e4617785df>:655: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ta_model.load_state_dict(torch.load('/content/resnet_34_tf.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
            "             ReLU-21           [-1, 64, 56, 56]               0\n",
            "           Conv2d-22           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 56, 56]             128\n",
            "             ReLU-24           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-25           [-1, 64, 56, 56]               0\n",
            "           Conv2d-26          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
            "             ReLU-28          [-1, 128, 28, 28]               0\n",
            "           Conv2d-29          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-30          [-1, 128, 28, 28]             256\n",
            "           Conv2d-31          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
            "             ReLU-37          [-1, 128, 28, 28]               0\n",
            "           Conv2d-38          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-39          [-1, 128, 28, 28]             256\n",
            "             ReLU-40          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-41          [-1, 128, 28, 28]               0\n",
            "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
            "             ReLU-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "             ReLU-47          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-55          [-1, 128, 28, 28]               0\n",
            "           Conv2d-56          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-57          [-1, 256, 14, 14]             512\n",
            "             ReLU-58          [-1, 256, 14, 14]               0\n",
            "           Conv2d-59          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-60          [-1, 256, 14, 14]             512\n",
            "           Conv2d-61          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-62          [-1, 256, 14, 14]             512\n",
            "             ReLU-63          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-64          [-1, 256, 14, 14]               0\n",
            "           Conv2d-65          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-66          [-1, 256, 14, 14]             512\n",
            "             ReLU-67          [-1, 256, 14, 14]               0\n",
            "           Conv2d-68          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-69          [-1, 256, 14, 14]             512\n",
            "             ReLU-70          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-71          [-1, 256, 14, 14]               0\n",
            "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
            "             ReLU-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "             ReLU-77          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "           Conv2d-79          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-80          [-1, 256, 14, 14]             512\n",
            "             ReLU-81          [-1, 256, 14, 14]               0\n",
            "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
            "             ReLU-84          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-85          [-1, 256, 14, 14]               0\n",
            "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
            "             ReLU-88          [-1, 256, 14, 14]               0\n",
            "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
            "             ReLU-91          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "           Conv2d-93          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
            "             ReLU-95          [-1, 256, 14, 14]               0\n",
            "           Conv2d-96          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-97          [-1, 256, 14, 14]             512\n",
            "             ReLU-98          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-99          [-1, 256, 14, 14]               0\n",
            "          Conv2d-100            [-1, 512, 7, 7]       1,179,648\n",
            "     BatchNorm2d-101            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-102            [-1, 512, 7, 7]               0\n",
            "          Conv2d-103            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-107            [-1, 512, 7, 7]               0\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "          Conv2d-109            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-110            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-111            [-1, 512, 7, 7]               0\n",
            "          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-114            [-1, 512, 7, 7]               0\n",
            "      BasicBlock-115            [-1, 512, 7, 7]               0\n",
            "          Conv2d-116            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-118            [-1, 512, 7, 7]               0\n",
            "          Conv2d-119            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-120            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-121            [-1, 512, 7, 7]               0\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
            "          Linear-124                  [-1, 200]         102,600\n",
            "================================================================\n",
            "Total params: 21,387,272\n",
            "Trainable params: 21,387,272\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 96.28\n",
            "Params size (MB): 81.59\n",
            "Estimated Total Size (MB): 178.44\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                  [-1, 200]         102,600\n",
            "================================================================\n",
            "Total params: 11,279,112\n",
            "Trainable params: 11,279,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 43.03\n",
            "Estimated Total Size (MB): 106.39\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-13          [-1, 128, 28, 28]             256\n",
            "             ReLU-14          [-1, 128, 28, 28]               0\n",
            "           Conv2d-15          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-16          [-1, 128, 28, 28]             256\n",
            "           Conv2d-17          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-18          [-1, 128, 28, 28]             256\n",
            "             ReLU-19          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-20          [-1, 128, 28, 28]               0\n",
            "           Conv2d-21          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-22          [-1, 256, 14, 14]             512\n",
            "             ReLU-23          [-1, 256, 14, 14]               0\n",
            "           Conv2d-24          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-25          [-1, 256, 14, 14]             512\n",
            "           Conv2d-26          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-27          [-1, 256, 14, 14]             512\n",
            "             ReLU-28          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-29          [-1, 256, 14, 14]               0\n",
            "           Conv2d-30            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-31            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-32            [-1, 512, 7, 7]               0\n",
            "           Conv2d-33            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-34            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-35            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-36            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-37            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-38            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-39            [-1, 512, 1, 1]               0\n",
            "           Linear-40                  [-1, 200]         102,600\n",
            "================================================================\n",
            "Total params: 5,000,712\n",
            "Trainable params: 5,000,712\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 42.69\n",
            "Params size (MB): 19.08\n",
            "Estimated Total Size (MB): 62.34\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets.folder import default_loader\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # Reduced convolution complexity\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, num_classes=200):\n",
        "        super(ResNet10, self).__init__()\n",
        "        block = BasicBlock\n",
        "\n",
        "        # Initial convolution layer with reduced kernel size\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Create layers with reduced depth\n",
        "        self.layer1 = self._make_layer(block, 64, 64, 1, stride=1)\n",
        "        self.layer2 = self._make_layer(block, 64, 128, 1, stride=2)\n",
        "        self.layer3 = self._make_layer(block, 128, 256, 1, stride=2)\n",
        "        self.layer4 = self._make_layer(block, 256, 512, 1, stride=2)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Final fully connected layer\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # Weight initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, in_channels, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(in_channels, out_channels, stride, downsample))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels * block.expansion, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Define paths\n",
        "data_dir = './tiny-imagenet-200'\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "val_dir = os.path.join(data_dir, 'val')\n",
        "\n",
        "# Download and extract Tiny ImageNet dataset\n",
        "def download_and_extract_tiny_imagenet():\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
        "        filename = 'tiny-imagenet-200.zip'\n",
        "        zip_path = os.path.join('./', filename)\n",
        "        print('Downloading Tiny ImageNet dataset...')\n",
        "        download_url(url, root='./', filename=filename)\n",
        "        print('Extracting Tiny ImageNet dataset...')\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('./')\n",
        "        os.remove(zip_path)\n",
        "        print('Dataset downloaded and extracted.')\n",
        "    else:\n",
        "        print('Tiny ImageNet dataset already exists.')\n",
        "\n",
        "download_and_extract_tiny_imagenet()\n",
        "\n",
        "# Prepare validation data\n",
        "def prepare_val_folder():\n",
        "    val_img_dir = os.path.join(val_dir, 'images')\n",
        "    if not os.path.exists(val_img_dir):\n",
        "        return\n",
        "    # Read val annotations file\n",
        "    val_annotations_file = os.path.join(val_dir, 'val_annotations.txt')\n",
        "    val_img_dict = {}\n",
        "    with open(val_annotations_file, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            parts = line.strip().split('\\t')\n",
        "            img_name = parts[0]\n",
        "            img_class = parts[1]\n",
        "            val_img_dict[img_name] = img_class\n",
        "\n",
        "    # Create folders for validation images\n",
        "    print('Organizing validation images...')\n",
        "    for img, cls in tqdm(val_img_dict.items()):\n",
        "        cls_dir = os.path.join(val_dir, cls)\n",
        "        if not os.path.exists(cls_dir):\n",
        "            os.mkdir(cls_dir)\n",
        "            os.mkdir(os.path.join(cls_dir, 'images'))\n",
        "        img_src = os.path.join(val_dir, 'images', img)\n",
        "        img_dst = os.path.join(cls_dir, 'images', img)\n",
        "        if os.path.exists(img_src):\n",
        "            os.rename(img_src, img_dst)\n",
        "    os.rmdir(os.path.join(val_dir, 'images'))\n",
        "    print('Validation images organized.')\n",
        "\n",
        "prepare_val_folder()\n",
        "\n",
        "# Define data transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create custom dataset class\n",
        "class TinyImageNetDataset(Dataset):\n",
        "    def __init__(self, root, train=True, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.train = train\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        if self.train:\n",
        "            data_dir = os.path.join(self.root, 'train')\n",
        "        else:\n",
        "            data_dir = os.path.join(self.root, 'val')\n",
        "        classes = sorted(os.listdir(data_dir))\n",
        "        class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
        "        for cls_name in classes:\n",
        "            cls_dir = os.path.join(data_dir, cls_name, 'images')\n",
        "            if not os.path.isdir(cls_dir):\n",
        "                continue\n",
        "            img_files = os.listdir(cls_dir)\n",
        "            for img_name in img_files:\n",
        "                img_path = os.path.join(cls_dir, img_name)\n",
        "                self.images.append(img_path)\n",
        "                self.labels.append(class_to_idx[cls_name])\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = default_loader(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = TinyImageNetDataset(root=data_dir, train=True, transform=transform_train)\n",
        "test_dataset = TinyImageNetDataset(root=data_dir, train=False, transform=transform_test)\n",
        "\n",
        "# Set a larger batch size\n",
        "batch_size = 1024  # Adjust this value based on your GPU memory\n",
        "print('Batch size:', batch_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Define function to create and modify ResNet models\n",
        "def create_resnet_model(name, num_classes=200, pretrained=True):\n",
        "    if name == 'resnet10':\n",
        "        model = ResNet10(num_classes=num_classes)\n",
        "    elif name == 'resnet18':\n",
        "        model = torchvision.models.resnet18(pretrained=pretrained)\n",
        "    elif name == 'resnet34':\n",
        "        model = torchvision.models.resnet34(pretrained=pretrained)\n",
        "    elif name == 'resnet101':\n",
        "        model = torchvision.models.resnet101(pretrained=pretrained)\n",
        "    else:\n",
        "        raise ValueError('Invalid model name')\n",
        "\n",
        "    # Modify the final layer to match num_classes\n",
        "    if name != 'resnet10':\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Function to train a model normally (used for teacher model)\n",
        "def train_model(model, train_loader, test_loader, num_epochs=10, base_lr=0.1, device='cuda', save_path='best_model.pth'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    adjusted_lr = base_lr * (batch_size / 256)\n",
        "    print('Adjusted learning rate:', adjusted_lr)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=adjusted_lr,\n",
        "                          momentum=0.9, weight_decay=5e-4)\n",
        "    scaler = GradScaler()\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    model.to(device)\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (batch_idx+1) % 10 == 0:\n",
        "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                      % (epoch+1, num_epochs, batch_idx+1, len(train_loader), running_loss/10))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                targets = targets.to(device, non_blocking=True)\n",
        "                with autocast():\n",
        "                    outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print('Test Accuracy of the model on the test images: {:.2f} %'.format(acc))\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Saved best model to {save_path}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Best Accuracy: {:.2f} %'.format(best_acc))\n",
        "    return best_acc\n",
        "\n",
        "# Function for knowledge distillation from teacher to student\n",
        "def train_kd(student_model, teacher_model, train_loader, test_loader, num_epochs=10, base_lr=0.1, temperature=4, alpha=0.9, device='cuda', save_path='best_student_model.pth'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    soft_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    adjusted_lr = base_lr * (batch_size / 256)\n",
        "    print('Adjusted learning rate:', adjusted_lr)\n",
        "\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=adjusted_lr,\n",
        "                          momentum=0.9, weight_decay=5e-4)\n",
        "    scaler = GradScaler()\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    student_model.to(device)\n",
        "    teacher_model.to(device)\n",
        "    teacher_model.eval()\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        student_model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = student_model(inputs)\n",
        "                with torch.no_grad():\n",
        "                    teacher_outputs = teacher_model(inputs)\n",
        "\n",
        "                loss_ce = criterion(outputs, targets)\n",
        "                loss_kd = soft_loss_fn(F.log_softmax(outputs/temperature, dim=1),\n",
        "                                       F.softmax(teacher_outputs/temperature, dim=1)) * (temperature ** 2)\n",
        "\n",
        "                loss = alpha * loss_kd + (1 - alpha) * loss_ce\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (batch_idx+1) % 10 == 0:\n",
        "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                      % (epoch+1, num_epochs, batch_idx+1, len(train_loader), running_loss/10))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation\n",
        "        student_model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                targets = targets.to(device, non_blocking=True)\n",
        "                with autocast():\n",
        "                    outputs = student_model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print('Test Accuracy of the student model on the test images: {:.2f} %'.format(acc))\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            # Save the best model\n",
        "            torch.save(student_model.state_dict(), save_path)\n",
        "            print(f\"Saved best model to {save_path}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Best Accuracy: {:.2f} %'.format(best_acc))\n",
        "    return best_acc\n",
        "\n",
        "# Function for knowledge distillation with both teacher and TA (simple average)\n",
        "def train_kd_with_ta(student_model, teacher_model, ta_model, train_loader, test_loader, num_epochs=10, base_lr=0.1, temperature=4, alpha=0.9, device='cuda', save_path='best_student_model.pth'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    soft_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    adjusted_lr = base_lr * (batch_size / 256)\n",
        "    print('Adjusted learning rate:', adjusted_lr)\n",
        "\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=adjusted_lr,\n",
        "                          momentum=0.9, weight_decay=5e-4)\n",
        "    scaler = GradScaler()\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    student_model.to(device)\n",
        "    teacher_model.to(device)\n",
        "    teacher_model.eval()\n",
        "    ta_model.to(device)\n",
        "    ta_model.eval()\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        student_model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = student_model(inputs)\n",
        "                with torch.no_grad():\n",
        "                    teacher_outputs = teacher_model(inputs)\n",
        "                    ta_outputs = ta_model(inputs)\n",
        "                    # Average the softmax outputs\n",
        "                    avg_outputs = (F.softmax(teacher_outputs/temperature, dim=1) + F.softmax(ta_outputs/temperature, dim=1)) / 2\n",
        "\n",
        "                loss_ce = criterion(outputs, targets)\n",
        "                loss_kd = soft_loss_fn(F.log_softmax(outputs/temperature, dim=1),\n",
        "                                       avg_outputs) * (temperature ** 2)\n",
        "\n",
        "                loss = alpha * loss_kd + (1 - alpha) * loss_ce\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (batch_idx+1) % 10 == 0:\n",
        "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                      % (epoch+1, num_epochs, batch_idx+1, len(train_loader), running_loss/10))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation\n",
        "        student_model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                targets = targets.to(device, non_blocking=True)\n",
        "                with autocast():\n",
        "                    outputs = student_model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print('Test Accuracy of the student model on the test images: {:.2f} %'.format(acc))\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            # Save the best model\n",
        "            torch.save(student_model.state_dict(), save_path)\n",
        "            print(f\"Saved best model to {save_path}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Best Accuracy: {:.2f} %'.format(best_acc))\n",
        "    return best_acc\n",
        "\n",
        "# Function for the new distillation algorithm\n",
        "def train_kd_new_algorithm(student_model, teacher_model, ta_model, train_loader, test_loader, num_epochs=10, base_lr=0.1, temp=5, alpha=0.9, device='cuda', save_path='best_student_model.pth'):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='none')  # per-sample loss\n",
        "    kl_criterion = nn.KLDivLoss(reduction='none')  # per-sample loss\n",
        "\n",
        "    adjusted_lr = base_lr * (batch_size / 256)\n",
        "    print('Adjusted learning rate:', adjusted_lr)\n",
        "\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=adjusted_lr,\n",
        "                          momentum=0.9, weight_decay=5e-4)\n",
        "    scaler = GradScaler()\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    student_model.to(device)\n",
        "    teacher_model.to(device)\n",
        "    teacher_model.eval()\n",
        "    ta_model.to(device)\n",
        "    ta_model.eval()\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        student_model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
        "            data = data.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                output = student_model(data)\n",
        "                with torch.no_grad():\n",
        "                    teacher_outputs = teacher_model(data)\n",
        "                    ta_outputs = ta_model(data)\n",
        "\n",
        "                # Standard Learning Loss (Classification Loss)\n",
        "                loss_SL = criterion(output, target)  # shape: [batch_size]\n",
        "                hard_loss = loss_SL\n",
        "\n",
        "                # Implement the new distillation algorithm\n",
        "                # Compute per-sample cross-entropy losses for teacher and TA\n",
        "                ce_teacher = criterion(teacher_outputs, target)  # shape: [batch_size]\n",
        "                ce_ta = criterion(ta_outputs, target)  # shape: [batch_size]\n",
        "\n",
        "                # Compute negative ce\n",
        "                neg_ce_teacher = -ce_teacher\n",
        "                neg_ce_ta = -ce_ta\n",
        "\n",
        "                # Stack negative ce to compute confidence scores\n",
        "                neg_ce = torch.stack([neg_ce_teacher, neg_ce_ta], dim=1)  # shape: [batch_size, 2]\n",
        "\n",
        "                # Compute confidence scores\n",
        "                conf_scores = F.softmax(neg_ce, dim=1)  # shape: [batch_size, 2]\n",
        "\n",
        "                conf_teacher = conf_scores[:, 0]  # shape: [batch_size]\n",
        "                conf_ta = conf_scores[:, 1]  # shape: [batch_size]\n",
        "\n",
        "                # Compute softmax outputs for teacher and TA\n",
        "                teacher_pred = F.softmax(teacher_outputs / temp, dim=1)  # shape: [batch_size, num_classes]\n",
        "                ta_pred = F.softmax(ta_outputs / temp, dim=1)\n",
        "\n",
        "                # Compute KL divergence between teacher and TA\n",
        "                kl_teacher_ta = kl_criterion(\n",
        "                    F.log_softmax(teacher_outputs / temp, dim=1),\n",
        "                    ta_pred\n",
        "                ).sum(dim=1)  # shape: [batch_size]\n",
        "\n",
        "                # Compute kl_factor\n",
        "                kl_factor = torch.sigmoid(kl_teacher_ta)  # shape: [batch_size]\n",
        "\n",
        "                # Compute final weights\n",
        "                w_teacher = (1 - kl_factor) * 0.5 + kl_factor * conf_teacher  # shape: [batch_size]\n",
        "                w_ta = (1 - kl_factor) * 0.5 + kl_factor * conf_ta  # shape: [batch_size]\n",
        "\n",
        "                # Compute KL divergence between student and teacher\n",
        "                kl_student_teacher = kl_criterion(\n",
        "                    F.log_softmax(output / temp, dim=1),\n",
        "                    teacher_pred\n",
        "                ).sum(dim=1)  # shape: [batch_size]\n",
        "\n",
        "                kl_student_ta = kl_criterion(\n",
        "                    F.log_softmax(output / temp, dim=1),\n",
        "                    ta_pred\n",
        "                ).sum(dim=1)  # shape: [batch_size]\n",
        "\n",
        "                # Compute soft losses\n",
        "                soft_loss_teacher = w_teacher * kl_student_teacher * (temp ** 2)  # shape: [batch_size]\n",
        "                soft_loss_ta = w_ta * kl_student_ta * (temp ** 2)  # shape: [batch_size]\n",
        "\n",
        "                # Compute total_loss per sample\n",
        "                total_loss = alpha * (soft_loss_teacher + soft_loss_ta) + (1 - alpha) * hard_loss  # shape: [batch_size]\n",
        "\n",
        "                # Compute loss as average over batch\n",
        "                loss = total_loss.mean()\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (batch_idx+1) % 10 == 0:\n",
        "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                      % (epoch+1, num_epochs, batch_idx+1, len(train_loader), running_loss/10))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation\n",
        "        student_model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                targets = targets.to(device, non_blocking=True)\n",
        "                with autocast():\n",
        "                    outputs = student_model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print('Test Accuracy of the student model on the test images: {:.2f} %'.format(acc))\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            # Save the best model\n",
        "            torch.save(student_model.state_dict(), save_path)\n",
        "            print(f\"Saved best model to {save_path}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Best Accuracy: {:.2f} %'.format(best_acc))\n",
        "    return best_acc\n",
        "\n",
        "# Set up device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "# Load the Teacher Model (ResNet-101)\n",
        "print('Loading Teacher Model (ResNet-101)')\n",
        "teacher_model = create_resnet_model('resnet101', num_classes=200, pretrained=False)\n",
        "teacher_model.load_state_dict(torch.load('/content/best_model.pth'))\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()\n",
        "\n",
        "summary(teacher_model, (3, 224, 224))\n",
        "\n",
        "print('Loading TA Model (ResNet-34)')\n",
        "ta_model = create_resnet_model('resnet34', num_classes=200, pretrained=False)\n",
        "ta_model.load_state_dict(torch.load('/content/resnet_34_tf.pth'))\n",
        "ta_model = ta_model.to(device)\n",
        "ta_model.eval()\n",
        "\n",
        "print(\"NO KD\")\n",
        "no_kd_10 = create_resnet_model('resnet10', num_classes=200, pretrained=False)\n",
        "no_kd_10 = no_kd_10.to(device)\n",
        "no_kd_10_best_acc = train_model(no_kd_10, train_loader, test_loader, num_epochs=40, base_lr=0.1, device=device, save_path='no_kd_10.pth')\n",
        "\n",
        "print(\"STANDARD KD\")\n",
        "standard_kd_10 = create_resnet_model('resnet10', num_classes=200, pretrained=False)\n",
        "standard_kd_10 = standard_kd_10.to(device)\n",
        "standard_kd_10_best_acc = train_kd(standard_kd_10, teacher_model, train_loader, test_loader, num_epochs=40, base_lr=0.1, temperature=4, alpha=0.9, device=device, save_path='standard_kd_10.pth')\n",
        "\n",
        "print(\"TA\")\n",
        "student_model_alg3 = create_resnet_model('resnet10', num_classes=200, pretrained=False)\n",
        "student_model_alg3 = student_model_alg3.to(device)\n",
        "student_best_acc_alg3 = train_kd(student_model_alg3, ta_model, train_loader, test_loader, num_epochs=40, base_lr=0.1, temperature=4, alpha=0.9, device=device, save_path='student_model_alg3.pth')\n",
        "\n",
        "print(\"Average\")\n",
        "student_model_alg2 = create_resnet_model('resnet10', num_classes=200, pretrained=False)\n",
        "student_model_alg2 = student_model_alg2.to(device)\n",
        "student_best_acc_alg2 = train_kd_with_ta(student_model_alg2, teacher_model, ta_model, train_loader, test_loader, num_epochs=40, base_lr=0.1, temperature=4, alpha=0.9, device=device, save_path='student_model_alg2.pth')\n",
        "\n",
        "print(\"Weighted\")\n",
        "student_model_alg1 = create_resnet_model('resnet10', num_classes=200, pretrained=False)\n",
        "student_model_alg1 = student_model_alg1.to(device)\n",
        "student_best_acc_alg1 = train_kd_new_algorithm(student_model_alg1, teacher_model, ta_model, train_loader, test_loader, num_epochs=40, base_lr=0.1, temp=5, alpha=0.9, device=device, save_path='student_model_alg1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PIbvA5mei37",
        "outputId": "9becf3ed-1f1f-4631-ae60-667eb39b8a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 54M\n",
            "-rw-r--r-- 1 root root  27M Dec  9 03:25 best_model.pth\n",
            "-rw-r--r-- 1 root root  26M Dec  9 03:25 resnet_34_tf.pth\n",
            "drwxr-xr-x 1 root root 4.0K Dec  5 14:24 sample_data\n",
            "drwxr-xr-x 5 root root 4.0K Dec  9 03:27 tiny-imagenet-200\n"
          ]
        }
      ],
      "source": [
        "!ls -lh"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}